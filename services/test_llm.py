import requests
import sys

BASE_URL = "http://127.0.0.1:11434/v1"  # LLM æœåŠ¡åœ°å€ ollama ç«¯å£11434ï¼ŒvLLM é€šå¸¸ä¸º 8000ï¼ŒLM Studio ä¸º 1234
API_KEY = "EMPTY"  # æœ¬åœ°éƒ¨ç½²é€šå¸¸ä¸éœ€è¦
MODEL_NAME = "qwen3:30b"  # ä½ çš„æ¨¡å‹å

def check_service():
    """æ£€æµ‹æœåŠ¡æ˜¯å¦å¯åŠ¨"""
    try:
        r = requests.get(f"{BASE_URL}/models", timeout=3)
        if r.status_code == 200:
            models = r.json()
            print("âœ… æœåŠ¡å·²å¯åŠ¨ï¼Œæ”¯æŒçš„æ¨¡å‹ï¼š")
            for m in models.get("data", []):
                print(" -", m.get("id"))
            return True
    except requests.exceptions.RequestException:
        return False
    return False

def test_llm():
    """æµ‹è¯•ä¸€æ¬¡ LLM ChatCompletion"""
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": "ç”¨50å­—è§£é‡Šé‡å­çº ç¼ "}],
        "max_tokens": 100,
        "temperature": 0.7
    }
    try:
        r = requests.post(
            f"{BASE_URL}/chat/completions",
            json=payload,
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=15
        )
        if r.status_code == 200:
            result = r.json()
            print("âœ… LLM API è¿”å›ç»“æœï¼š")
            print(result["choices"][0]["message"]["content"])
        else:
            print(f"âŒ API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {r.status_code}, å†…å®¹: {r.text}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å‡ºé”™: {e}")

if __name__ == "__main__":
    if check_service():
        print("\nğŸš€ å¼€å§‹æµ‹è¯• LLM æ¨ç†æ¥å£...\n")
        test_llm()
    else:
        print("âŒ LLM æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æ¨ç†æœåŠ¡ã€‚")
        sys.exit(1)
