import asyncio
import aiohttp
import time
import numpy as np
from typing import List, Dict

# ===== é…ç½®åŒº =====
API_BASE_URL = "http://localhost:8000/v1"  # Ollamaé»˜è®¤ç«¯å£ï¼ŒvLLMé€šå¸¸ä¸º8000ï¼ŒLM Studioä¸º1234
API_KEY = "EMPTY"  # æœ¬åœ°éƒ¨ç½²é€šå¸¸æ— éœ€å¯†é’¥
MODEL_NAME = "ms/model/Qwen2.5-1.5B-Instruct"  # å®é™…éƒ¨ç½²çš„æ¨¡å‹åç§°
REQUEST_PAYLOAD = {
    "model": MODEL_NAME,
    "messages": [{"role": "user", "content": "ç”¨50å­—è§£é‡Šé‡å­çº ç¼ "}],
    "max_tokens": 100,
    "temperature": 0.7
}
CONCURRENCY_LEVELS = [1, 5, 10]  # æµ‹è¯•çš„å¹¶å‘ç”¨æˆ·æ•°
REQUESTS_PER_CLIENT = 20  # æ¯ä¸ªå®¢æˆ·ç«¯å‘é€çš„è¯·æ±‚æ•°
# ==================

async def send_request(session: aiohttp.ClientSession, payload: Dict) -> Dict:
    """å‘é€å•æ¬¡è¯·æ±‚å¹¶è®°å½•å»¶è¿Ÿ"""
    start_time = time.perf_counter()
    try:
        async with session.post(
            f"{API_BASE_URL}/chat/completions",
            json=payload,
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=30
        ) as response:
            if response.status == 200:
                result = await response.json()
                latency = (time.perf_counter() - start_time) * 1000  # æ¯«ç§’
                return {"success": True, "latency": latency}
            return {"success": False, "error": f"HTTP {response.status}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

async def client_task(session: aiohttp.ClientSession, results: List, client_id: int):
    """å•ä¸ªå®¢æˆ·ç«¯ä»»åŠ¡ï¼šå‘é€å¤šè½®è¯·æ±‚"""
    for _ in range(REQUESTS_PER_CLIENT):
        result = await send_request(session, REQUEST_PAYLOAD)
        results.append(result)

async def run_test(concurrency: int):
    """æ‰§è¡ŒæŒ‡å®šå¹¶å‘çº§åˆ«çš„æµ‹è¯•"""
    results = []
    async with aiohttp.ClientSession() as session:
        tasks = [asyncio.create_task(client_task(session, results, i)) 
                 for i in range(concurrency)]
        await asyncio.gather(*tasks)
    return results

def calculate_metrics(results: List) -> Dict:
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    latencies = [r["latency"] for r in results if r.get("success")]
    errors = [r for r in results if not r["success"]]
    
    return {
        "total_requests": len(results),
        "success_rate": len(latencies) / len(results) * 100,
        "avg_latency_ms": np.mean(latencies) if latencies else 0,
        "p95_latency_ms": np.percentile(latencies, 95) if latencies else 0,
        "throughput_rps": len(latencies) / (max(latencies)/1000) if latencies else 0,
        "error_types": {e["error"]: sum(1 for r in errors if r["error"] == e["error"]) 
                       for e in errors}
    }

async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print(f"ğŸ å¼€å§‹æ€§èƒ½æµ‹è¯• | æ¨¡å‹: {MODEL_NAME} | æ¥å£: {API_BASE_URL}")
    print("-" * 65)
    
    for concurrency in CONCURRENCY_LEVELS:
        start_time = time.perf_counter()
        results = await run_test(concurrency)
        duration = time.perf_counter() - start_time
        metrics = calculate_metrics(results)
        
        # æ‰“å°æµ‹è¯•ç»“æœ
        print(f" å¹¶å‘ [{concurrency}] | è¯·æ±‚ [{metrics['total_requests']}]")
        print(f" æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
        print(f" å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency_ms']:.1f} ms")
        print(f" P95å»¶è¿Ÿ: {metrics['p95_latency_ms']:.1f} ms")
        print(f" ååé‡: {metrics['throughput_rps']:.1f} è¯·æ±‚/ç§’")
        
        if metrics["error_types"]:
            print(f"  âŒ é”™è¯¯åˆ†å¸ƒ: {metrics['error_types']}")
        print("-" * 65)

if __name__ == "__main__":
    asyncio.run(main())