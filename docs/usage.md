## 1. 添加你的AI模型

<!-- ![资源选项卡](images/resources-tab.png) -->

- 该工具使用大型语言模型（LLMs）在QA流程中执行各种任务。
  因此，你需要为应用程序提供访问你想使用的LLMs的权限。
- 你只需要提供至少一个模型。然而，建议你包括所有你有权限访问的LLMs，这样你就可以在使用应用程序时在它们之间切换。

要添加模型：

1. 导航到 `资源管理` 选项卡。
2. 选择 `大语言模型` 子选项卡。
3. 选择 `Add` 子选项卡。
4. 配置要添加的模型：
   - 给它命名。
   - 选择供应商/提供者（例如 `ChatOpenAI`）。
   - 提供规格。
   - （可选）将模型设置为默认。
5. 点击 `Add LLM` 以添加模型。
6. 选择 `嵌入模型` 子选项卡并重复步骤3到5以添加嵌入模型。

<details markdown>

<summary>（可选）通过.env文件配置模型</summary>

或者，你可以通过`.env`文件配置模型，该文件包含连接到LLMs所需的信息。该文件位于应用程序的文件夹中。如果你没有看到它，可以创建一个。

目前，支持以下提供者：

### OpenAI

在`.env`文件中，设置`OPENAI_API_KEY`变量为你的OpenAI API密钥，以启用对OpenAI模型的访问。还有其他变量可以修改，请随意编辑以适合你的情况。否则，默认参数应该适合大多数人。

```shell
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=<你的OpenAI API密钥>
OPENAI_CHAT_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
```

### Azure OpenAI

对于通过Azure平台的OpenAI模型，你需要提供你的Azure端点和API密钥。你可能还需要提供你的开发名称以用于聊天模型和嵌入模型，具体取决于你如何设置Azure开发。

```shell
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
OPENAI_API_VERSION=2024-02-15-preview # 可能与你的不同
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo # 更改为你的部署名称
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002 # 更改为你的部署名称
```

### 本地模型

优点：

- 隐私。你的文档将被存储和处理在本地。
- 选择。可以选择各种大小、领域、语言的LLMs。
- 成本。免费。

缺点：

- 质量。本地模型较小，因此生成质量低于付费API。
- 速度。本地模型使用你的机器进行部署，因此处理速度受限于你的硬件。

#### 查找和下载LLM

你可以从[Hugging Face Hub](https://huggingface.co/models)搜索和下载可以在本地运行的LLM。目前支持以下模型格式：

- GGUF

你应该选择一个大小小于设备内存的模型，并应保留大约2 GB。例如，如果你总共有16 GB的RAM，其中12 GB可用，那么你应该选择一个最多占用10 GB RAM的模型。较大的模型往往生成更好，但也需要更多的处理时间。

以下是一些推荐及其在内存中的大小：

- [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true)：大约2 GB

#### 启用本地模型

要将本地模型添加到模型池中，请在`.env`文件中设置`LOCAL_MODEL`变量为模型文件的路径。

```shell
LOCAL_MODEL=<你的模型文件的完整路径>
```

以下是获取模型文件完整路径的方法：

- 在Windows 11上：右键单击文件并选择`复制为路径`。
</details>

## 2. 上传你的文档

<!-- ![文件索引选项卡](images/file-index-tab.png) -->

为了对你的文档进行QA，你需要先将它们上传到应用程序。导航到`文件索引`选项卡，你将看到两个部分：

1. 文件上传：
   - 将文件拖放到UI中或从文件系统中选择。然后点击`上传并索引`。
   - 应用程序将花费一些时间处理文件，并在完成后显示消息。
2. 文件列表：
   - 此部分显示已上传到应用程序的文件列表，并允许用户删除它们。

## 3. 与文档聊天
<!--  -->
<!-- ![聊天选项卡](images/chat-tab.png) -->

现在返回到`聊天`选项卡。聊天选项卡分为三个区域：

1. 会话设置面板
   - 在这里你可以选择、创建、重命名和删除会话。
     - 默认情况下，如果没有选择会话，将自动创建一个新会话。
   - 在其下方是文件索引，你可以选择禁用、选择所有文件或选择要从中检索引用的文件。
     - 如果选择“禁用”，则在聊天期间不会将任何文件视为上下文。
     - 如果选择“搜索所有”，则在聊天期间将考虑所有文件。
     - 如果选择“选择”，将出现一个下拉菜单供你选择在聊天期间要考虑的文件。如果没有选择文件，则在聊天期间不会考虑任何文件。
2. 聊天面板
   - 这是你可以与聊天机器人聊天的地方。
3. 信息面板

<!-- ![信息面板](images/info-panel-scores.png) -->

- 支持信息如检索到的证据和参考将在此显示。
- LLM生成的答案的直接引用被突出显示。
- 答案的置信度分数和证据的相关分数显示在此，以快速评估答案和检索内容的质量。

- 显示的分数含义：
  - **答案置信度**：来自LLM模型的答案置信度水平。
  - **相关性分数**：证据与用户问题之间的整体相关性分数。
  - **向量存储分数**：来自向量嵌入相似性计算的相关分数（如果从全文搜索数据库中检索，则显示“全文搜索”）。
  - **LLM相关分数**：来自LLM模型的相关分数（使用特定提示判断问题和证据之间的相关性）。
  - **重新排序分数**：来自Cohere [重新排序模型](https://cohere.com/rerank)的相关分数。

通常，分数质量为`LLM相关分数` > `重新排序分数` > `向量分数`。
默认情况下，整体相关性分数直接取自LLM相关分数。证据根据其整体相关性分数和是否有引用进行排序。
